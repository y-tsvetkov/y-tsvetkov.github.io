<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Maah placeholder | Yordan Tsvetkov</title> <meta name="author" content="Yordan Tsvetkov"> <meta name="description" content="in both simulation and reality"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%A4%96&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="https://y-tsvetkov.github.io/TempProjects/4_project/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js"></script> <script src="/assets/js/dark_mode.js"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Yordan </span>Tsvetkov</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about</a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">Maah placeholder</h1> <p class="post-description">in both simulation and reality</p> </header> <article> <p>This project was done back in 2019-2020 as part of a national competition. TL;DR: you can get a robot to walk fairly robustly with periodic neural networks about 100x smaller than their non-periodic eqivalents.</p> <h2 id="inspiration">Inspiration</h2> <p>The idea behind the neural network architecture comes from two biological concepts, which suggest something very interesting: walking can be expressed as a low-dimensional sum of periodic signals.</p> <h3 id="central-pattern-generators">Central Pattern Generators</h3> <p>In both vertebrate and invertebrate animals, neural signals of a periodic nature are controlled by a particular type of neural circuits known as central pattern generators (CPGs). They govern respiratory and vegetative processes (breathing, chewing, swallowing, gut movements) as well as locomotion (walking, crawling, swimming, flying) in most animals and heartbeat in some invertebrates. Their main characteristic is the ability to generate <em>rhythmic outputs from non-rhythmic inputs</em>.</p> <p>Such constructs are also used for the control of walking robots, where the CPGs are based on coupled nonlinear oscillators such as Van der Pol oscillators, which converge to a stable periodic motion and thus they ensure stable motion regardless of the initial state. The robots I’ve seen using them are usually sensorless, with the non-rhytmic input being just time. They can still yield some impressive results - <a href="https://www.researchgate.net/publication/256483927_Towards_Dynamic_Trot_Gait_Locomotion_Design_Control_and_Experiments_with_Cheetah-cub_a_Compliant_Quadruped_Robot" rel="external nofollow noopener" target="_blank">one of the fastest small quadrupeds, EPFL’s Cheetah-Cub, used 1 oscillator per joint for great effect.</a></p> <h3 id="kinematic-motion-primitives">Kinematic Motion Primitives</h3> <p>It turns out that complex movements performed by animals can be expressed as periodic signals with reduced dimensionality referred to as Kinematic Motion Primitives (kMPs). <a href="https://www.researchgate.net/publication/235797562_Horse-like_walking_trotting_and_galloping_derived_from_kinematic_Motion_Primitives_kMPs_and_their_application_to_walktrot_transitions_in_a_compliant_quadruped_robot" rel="external nofollow noopener" target="_blank">As this paper by Moro et al. shows</a>, using principal component analysis applied to the joint data of a walking horse shows that 4 particular signals are enough to account for 97% of the variance in the joints in motion.</p> <p>After those kMPs are extracted, joint trajectories for a robot with different properties were derived from them by packing them up in a vector, multiplying them by a weight matrix and adding an offset vector. Pretty similar to a linear neural network layer. The resulting gaits were again deployed on Cheetah-Cub to great effect. But let’s take a look at those KMPs for a second:</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/kmp_graphs-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/kmp_graphs-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/kmp_graphs-1400.webp"></source> <img src="/assets/img/kmp_graphs.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>Interesting. They turn out to be pretty close to sines with varying frequencies. This seems to suggest that maybe you could get some pretty good walking from <em>linear combos of sines</em>.</p> <p>In short, we have 2 controllers which produce fairly good results with a few periodic signals. One of them gets such results by practically plugging the signal in a single-layer linear neural net. I don’t have a horse in a mocap suit on hand… but those signals resemble sines a lot.</p> <p><em>What if I train a neural network with a sine activation function to linearly combine a bunch of sine signals?</em></p> <h2 id="software">Software</h2> <p>The approach to seeing what a tiny network that does periodic signals can do was to train it in simulation using CMA-ES and then just transfer that to the real world.</p> <h3 id="neural-net-architecture">Neural net architecture</h3> <p>I just used a simple feedforward neural network with the following structure:</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/sine_nn-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/sine_nn-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/sine_nn-1400.webp"></source> <img src="/assets/img/sine_nn.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>The inputs are just the IMU pitch and roll, the angular velocities in the same axes and time (to make the sine oscillate in the first place). We can modulate the sine frequency and phase in the sine layer, and its amplitutde and offset in the linear layer, meaning the training algorithm can fully modulate the oscillation.</p> <h3 id="training-algorithm">Training algorithm</h3> <p>Since the networks I would be running did not have that many parameters, I settled on using CMA-ES, <a href="https://blog.otoro.net/2017/11/12/evolving-stable-strategies/" rel="external nofollow noopener" target="_blank">which has been used on some pretty difficult environments before.</a> The behavior of CMA-ES can be described in the following manner for a problem with \(n\) parameters (in our case, the weights and biases of the network) and a population of \(\lambda\):</p> <ul> <li>In every generation, \(\lambda\) vectors are sampled, each composed of \(n\) elements: \(\boldsymbol{x}_k^{g+1},k\in(1,\ldots,\lambda)\). These vectors with parameters are taken from a multivariate normal distribution \(\mathcal{N}(\boldsymbol{m}^g,\boldsymbol{C}^g)\), where \(g\) is the present generation, \(\boldsymbol{m}^g\) - the mean of the distribution, and \(\boldsymbol{C}^g\) - the covariance matrix:</li> </ul> \[\boldsymbol{x}_k^{g+1} \sim N(\boldsymbol{m}^g,\boldsymbol{C}^g ), k \in(1,\ldots,\lambda)\] <ul> <li>After all rollouts of a generation have been completed, \(\mu\) of the best performing vectors are selected for calculating the new mean \(\boldsymbol{m}^{g+1}\), where \(w_i\) is the normalised reward, for which \(\sum_{i=1}^{\mu}w_i=1\):</li> </ul> \[\boldsymbol{m}^{g+1}= \boldsymbol{m}^g+\sum_{i=1}^{\mu}w_i(\boldsymbol{x}_i^{g+1}-\boldsymbol{m}^g)\] <ul> <li>After that, the covariance matrix is recalculated using the mean of the present generation, which results in a matrix skewed towards the direction of the best performing solutions:</li> </ul> \[\boldsymbol{C}^{g+1}=\sum_{i=1}^{\mu}w_i (\boldsymbol{x}_i^{g+1}-\boldsymbol{m}^g)(\boldsymbol{x}_i^{g+1}-\boldsymbol{m}^g)^T\] <p>The result of all this math is that the area of possible solutions gets squished in the most promising direction, as shown in <a href="https://blog.otoro.net/2017/10/29/visual-evolution-strategies/" rel="external nofollow noopener" target="_blank">the excellent animation by David Ha</a>. The blue dots are all vectors, the red dot is the best performing vector \(\boldsymbol{x}\), the green dot is the mean \(\boldsymbol{m}^{g}\) and the purple dots are the top \(\mu\) vectors. The objective is the global maximum, denoted by the white area:</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cmaes_by_otoro.gif-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cmaes_by_otoro.gif-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cmaes_by_otoro.gif-1400.webp"></source> <img src="/assets/img/cmaes_by_otoro.gif" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>For more explanations of other evolution strategies involving a simple 2D use case, <a href="https://blog.otoro.net/2017/10/29/visual-evolution-strategies/" rel="external nofollow noopener" target="_blank">make sure to visit David Ha’s blog</a>.</p> <p>The hyperparameters that must be specified are just the standard deviation of the first generation \(\sigma\) and the population size (the original paper provides recommended values for the rest of the hyperparameters). This algorithm possesses a relatively big time complexity (\(O(n^2)\)), but the evaluation of the population takes much more time than the covariance matrix calculation, which combined with the small number of parameters of the network (due to the small number of neurons) means that this task is suited for CMA-ES optimisation.</p> <p>As described above, CMA-ES needs some sort of a reward function to evaluate how well a particular set of parameters does. This brings us to:</p> <h3 id="reward-function">Reward Function</h3> <p>At first, I tried just using the distance travelled as the fitness function, but was having trouble with the robot just falling forwards:</p> <iframe width="775" height="436" src="https://www.youtube.com/embed/X8imagA47oM" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe> <p>This local maximum was likely due to the low motor torque making walking fairly difficult, resulting in the robot just maximising the initial reward. The reward function needed to be something incentivising stable walking, so I used this instead:</p> \[f(t)=D\text{ sign}(\boldsymbol{V}_{avg})\mid\frac{\boldsymbol{V}_{avg}^n}{\boldsymbol{V}_{max}}\mid\cdot\boldsymbol{d}\] <p>Here \(V_{avg}\) and \(V_{max}\) represent the average and maximal velocity of the body. The significance factor \(n\) defines intuitively the balance between speed and stable motion. A higher \(n\) value causes the agent to prioritise speed, but at the expense of even motion. \(\boldsymbol{d}\) is a vector that defines the desired direction of movement and \(D\) is a reward scaling factor. As ES algorithms that take the value of the fitness of every individual instead of their ranking within a generation are affected by its magnitude, changing the reward factor can have an effect on the learning rate. \</p> <p>The factor \(V_{avg}/V_{max}\) has a maximal value for motion without any acceleration, i.e. \(V_{avg} = V_{max}\). The agent will still be incentivised to learn a policy that enables it to move faster if the significance factor \(n\) is bigger than 1.</p> <p>With this reward function, CMA-ES now ranks neural net parameters not just by the forward movement, but by its evenness as wel, and it had a much easier time producing walking gaits - all policies in the results used this one.</p> <h3 id="training">Training</h3> <p>I made an URDF of the physical robot (more on this later) using the inertial data from Fusion 360 and a PyBullet-based environment compatible with the <a href="https://github.com/openai/gym" rel="external nofollow noopener" target="_blank">OpenAI Gym</a> framework (this was before it was discontinued). In order to make the policy robust to the sim-to-real gap, I randomised a bunch of things, here’s the slide about them from my presentation on the project:</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/regularisation-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/regularisation-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/regularisation-1400.webp"></source> <img src="/assets/img/regularisation.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>For most of them, the idea came from this paper by <a href="https://arxiv.org/abs/1804.10332" rel="external nofollow noopener" target="_blank">Tan et al.</a> (I think this is where ‘sim-to-real’ came as a term too). After about 600 generations (at which point the robot can somewhat walk), the simulation applied forces of 70-100 N as perturbations that would tip the robot over unless it regains its balance. Finally, the knee joint range and maximal frequency had to be limited to prevent the robot sliding across the terrain by vibrating:</p> <iframe width="775" height="436" src="https://www.youtube.com/embed/s3qyXomqhkg" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe> <p>During training, I was running 6 parallel simulations using <code class="language-plaintext highlighter-rouge">multiprocessing.Pool</code>, 1 for each core of my laptop. It took between 1000-2000 generations to create working gaits, which was about a night of training on my Acer Aspire 7.</p> <h3 id="sim-results">Sim Results</h3> <p>The robot managed to learn a perturbation resistant rough terrain gait. A fun little emerging behaviour is that the robot stops and takes half a step backwards if it’s stuck on an obstacle it can’t cross. Here the network uses just 8 neurons:</p> <iframe width="775" height="436" src="https://www.youtube.com/embed/8t46JD0eP2E" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe> <iframe width="775" height="436" src="https://www.youtube.com/embed/ZB11xuW3Y5A" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe> <p>Adding the yaw as an input and bumping the neuron count to 12 even allowed the robot to do slight corrections to its course:</p> <iframe width="775" height="436" src="https://www.youtube.com/embed/qXE2g7E2N9s" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe> <h2 id="hardware">Hardware</h2> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/old_quad-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/old_quad-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/old_quad-1400.webp"></source> <img src="/assets/img/old_quad.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>The quadruped was built with the intent to make it easy to train, meaning stubby legs and dividing it into homogenous segments to save on URDF objects and thus give my laptop at least <em>some</em> chance of surviving the training process. There were some cute details to it, like the slots on top for cables for a harness that never got built, the fairly clean cable management, and the fact that all screws are easily accessible.</p> <p>The design did have some problems though. The desire to have the hip and knee servos together resulted in a weird curved four-bar arm which had some undesirable compliance. The legs could have been simpler too. I generated the parts for them using topology optimisation with a symmetry constraint and about 5 load cases from different directions. However, Fusion 360 outputs a mesh rather than a solid object, leading to some lofts which should’ve resulted the revocation of my education license. At least the topology optimisation resulted in geometry that I can now evaluate as ‘making sense’ - most mass was distributed away from the neutral bending axes, meaning they were fairly stiff (aside from the four-bar arm). Still, all these lessons were quite useful for future projects.</p> <p>As for electronics, I used <a href="https://hitecrcd.com/products/servos/analog/sport-2/hs-645mg/product" rel="external nofollow noopener" target="_blank">Hitec HS-645MGS</a> for actuation, a <a href="https://www.pjrc.com/store/teensy35.html" rel="external nofollow noopener" target="_blank">Teensy 3.5</a> for the brains and a <a href="https://uk.robotshop.com/products/bno055-9-dof-absolute-orientation-imu-module" rel="external nofollow noopener" target="_blank">BNO055 IMU</a> for sensing, all powered by a 2S Li-po connected to a <a href="https://hobbyking.com/en_us/yep-20a-hv-2-12s-sbec-w-selectable-voltage-output.html?___store=en_us" rel="external nofollow noopener" target="_blank">20A SBEC</a> for powering everyting at a consistent 5V.</p> <p>All electronics were powered to the SBEC for power using a custom PDB with a neat safety feature. The SBEC came with dual output leads, each of which ended with a generic 3-pin servo connector with a power, ground and an empty slot. I had the PDB power input pins next to each other, with the empty pins on both ends. If one connector was flipped, only a single wire would be connected to a pin and the circuit will remain open, meaning it’s not possible to short the circuit and brick everything.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/pcb-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/pcb-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/pcb-1400.webp"></source> <img src="/assets/img/pcb.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Custom PDB The FSR and Bluetooth module slots were for a later expansion of the robot. </div> <h2 id="sim-to-real">Sim-to-real</h2> <p>I plugged the weights and biases of the rough-terrain policy in an Arduino implementation of the net. The code ran quickly enough to match the PyBullet step frequency (240 Hz), which is important as simulation-trained NNs are very sensitive to latency(as described in the Tan et al. paper). Here’s the end result deployed on the robot:</p> <iframe width="775" height="436" src="https://www.youtube.com/embed/0eE_ecmASUU" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe> <p>The controller showed a pretty funny case of overfitting - since the simulation spawned the robot slightly above the ground each time, it learned to somehow use the downward acceleration as a signal to start walking. This meant I had to give it a bit of velocity downwards to do so, but it still walked fine otherwise, for something with no sim-to-real adjustments. For comparison with the state of the art back then, <a href="https://arxiv.org/abs/1804.10332" rel="external nofollow noopener" target="_blank">a conventional neural netwrork needs 2 hidden layers of 125 and 89 neurons respectively after hyperparameter optimisation</a>. The policy you are seeing here used 8.</p> <h2 id="whats-next">What’s next</h2> <p>Nothing right now, as this was done a while ago. The robot lived a fulfiling life entertaining friends and house guests until it was scavenged for its electronic components for various other robots. As for the sine NNs, it would be good to further explore what they can do. Maybe at some point in the future.</p> </article> </div> </div> <footer class="sticky-bottom mt-5"> <div class="container"> © Copyright 2024 Yordan Tsvetkov. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a>. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js"></script> <script defer src="/assets/js/common.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>